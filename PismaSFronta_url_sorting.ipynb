{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Url_sorting.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO2crBEwJ4N8OfYUuYHLUqW"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaMahk44TTeT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import lxml.etree as etree\n",
        "import urllib.request\n",
        "import os.path\n",
        "import re\n",
        "import lxml.html\n",
        "import csv\n",
        "import signal\n",
        "import requests"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1j-b8D2Tb-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#открываем файл со ссылками. Всего 1475 ссылок, не считая errors - отдельный файл\n",
        "with open('/content/drive/My Drive/ВКР/pisma_urs1.txt', 'r', encoding='utf-8') as file:\n",
        "  urls = file.readlines()\n",
        "  print(type(urls))\n",
        "  print(len(urls))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZMWX8RkTo5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ЭТАП 1 - отсеиваем по заголовкам\n",
        "\n",
        "ignore_head = ['лицо победы', 'из книги', 'из воспоминаний', 'дневник', 'из статьи', 'из приказа', 'из письма'] #список стоп-слов\n",
        "ignore_links = [] #список для ссылок, которые игнорим\n",
        "good_links = [] #список ссылок, которые обрабатываем + здесь отсеить по тэгам\n",
        "errors = []\n",
        "for url in urls:\n",
        "  try:\n",
        "    url = url.replace('\\n', '')\n",
        "    r = requests.get(url)\n",
        "    page_source = r.text\n",
        "    soup = BeautifulSoup(page_source, 'lxml')\n",
        "    head = soup.find('strong').text\n",
        "    lst = []\n",
        "    if any(substr in head.lower() for substr in ignore_head):\n",
        "      ignore_links.append(url)\n",
        "    else:\n",
        "      good_links.append(url)\n",
        "  except:\n",
        "    errors.append(url)\n",
        "\n",
        "print('Хороших ссылок на 1 этапе: '+ str(len(good_links)))\n",
        "print('Плохих ссылок на 1 этапе: '+ str(len(ignore_links)))\n",
        "print('Ошибок на 1 этапе: '+ str(len(errors)))\n",
        "\n",
        "\n",
        "#ЭТАП 2 получившийся список отсеиваем по тэгам, пополняем спиоск ignore, список хороших ссылок делаем новый\n",
        "\n",
        "ignore_tag = ['воспоминания', 'дневник', 'газета', 'письмо в редакцию', 'письмо в сельсовет', 'немцы']\n",
        "good_links2 = []\n",
        "for url in good_links:\n",
        "  try:\n",
        "    url = url.replace('\\n', '')\n",
        "    r = requests.get(url)\n",
        "    page_source = r.text\n",
        "    soup = BeautifulSoup(page_source, 'lxml')\n",
        "    full_text = soup.find('div', {'class':'flex_display flex_vbox'})\n",
        "    sub_headings = []\n",
        "    tags = []\n",
        "    for sub_heading in full_text.find_all('p'):\n",
        "      sub_headings.append(sub_heading.text)\n",
        "      for prgrf in sub_headings:\n",
        "        if prgrf == 'Tags:':\n",
        "          tags = ' '.join(sub_headings[sub_headings.index(prgrf):])\n",
        "          \n",
        "    if any(substr in tags.lower() for substr in ignore_tag):\n",
        "      ignore_links.append(url)\n",
        "    else:\n",
        "      good_links2.append(url)\n",
        "  except AttributeError:\n",
        "    errors_tag.append(url)\n",
        "\n",
        "print('Хороших ссылок на 2 этапе: '+ str(len(good_links2)))\n",
        "print('Плохих ссылок на 2 этапе: '+ str(len(ignore_links)))\n",
        "print('Ошибок на 2 этапе: '+ str(len(errors)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl4KtNsdTs5J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#тоже самое делаем со ссылками из файла с ошибками (316 ссылок)\n",
        "\n",
        "with open('/content/drive/My Drive/ВКР/pisma_errors.txt') as e:\n",
        "  errors = e.read()\n",
        "\n",
        "errors = errors.split(sep='Error: ')\n",
        "print(errors)\n",
        "\n",
        "errors_list = []\n",
        "for i in errors:\n",
        "  q = i.split(sep=' : ')\n",
        "  errors_list.append(q[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHlfmnr3T4qW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "good_links = []\n",
        "for url in errors_list:\n",
        "  try:\n",
        "    url = url.replace('\\n', '')\n",
        "    r = requests.get(url)\n",
        "    page_source = r.text\n",
        "    soup = BeautifulSoup(page_source, 'lxml')\n",
        "    head = soup.find('strong').text\n",
        "    lst = []\n",
        "    if any(substr in head.lower() for substr in ignore_head):\n",
        "      ignore_links.append(url)\n",
        "    else:\n",
        "      good_links.append(url)\n",
        "  except:\n",
        "    errors.append(url)\n",
        "\n",
        "print('Хороших ссылок на 1 этапе: '+ str(len(good_links)))\n",
        "print('Плохих ссылок на 1 этапе: '+ str(len(ignore_links)))\n",
        "print('Ошибок на 1 этапе: '+ str(len(errors)))\n",
        "\n",
        "ignore_tag = ['воспоминания', 'дневник', 'газета', 'письмо в редакцию', 'письмо в сельсовет', 'немцы']\n",
        "for url in good_links:\n",
        "  try:\n",
        "    url = url.replace('\\n', '')\n",
        "    r = requests.get(url)\n",
        "    page_source = r.text\n",
        "    soup = BeautifulSoup(page_source, 'lxml')\n",
        "    full_text = soup.find('div', {'class':'flex_display flex_vbox'})\n",
        "    sub_headings = []\n",
        "    tags = []\n",
        "    for sub_heading in full_text.find_all('p'):\n",
        "      sub_headings.append(sub_heading.text)\n",
        "      for prgrf in sub_headings:\n",
        "        if prgrf == 'Tags:':\n",
        "          tags = ' '.join(sub_headings[sub_headings.index(prgrf):])\n",
        "          \n",
        "    if any(substr in tags.lower() for substr in ignore_tag):\n",
        "      ignore_links.append(url)\n",
        "    else:\n",
        "      good_links2.append(url)\n",
        "  except AttributeError:\n",
        "    errors_tag.append(url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIZI280sULYn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#делим хорошие ссылки на три списка по количеству категорий\n",
        "letters_one_cat = []\n",
        "letters_sev_cat = []\n",
        "letters_no_cat = []\n",
        "\n",
        "#разбиваем все письма по категориям\n",
        "for url in good_links2:\n",
        "  r = requests.get(url)\n",
        "  page_source = r.text\n",
        "  soup = BeautifulSoup(page_source, 'lxml')\n",
        "#создаем список категорий. Разделяем на две группы: с одной категорией, с несколькими категориями  \n",
        "  category_list = []\n",
        "  for i in soup.find_all('div', {'class': 'style-j15atrbplink'}):\n",
        "    category_list.append(i.find('span').text)\n",
        "  if len(category_list) == 1:\n",
        "    letters_one_cat.append(url)\n",
        "  if len(category_list) == 0:\n",
        "    letters_no_cat.append(url)   #ВАЖНО!!!! Часть страниц, действительно, без категории!\n",
        "  if len(category_list) > 1:\n",
        "    letters_sev_cat.append(url) \n",
        "\n",
        "print('Одна категория:' + str(len(letters_one_cat))) \n",
        "print('Несколько категорий:' + str(len(letters_sev_cat))) \n",
        "print('Нет категорий:' + str(len(letters_no_cat))) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CkAjVzpUbZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#сохраняем все в файлики, чтобы не потерять\n",
        "\n",
        "with open('pisma_good_links1.txt', 'a', encoding='utf-8') as file:\n",
        "    for i in letters_one_cat:\n",
        "        file.write(\"%s\\n\" % i)\n",
        "\n",
        "with open('pisma_good_links2.txt', 'a', encoding='utf-8') as file:\n",
        "    for i in letters_sev_cat:\n",
        "        file.write(\"%s\\n\" % i)\n",
        "\n",
        "with open('pisma_good_links0.txt', 'a', encoding='utf-8') as file:\n",
        "    for i in letters_no_cat:\n",
        "        file.write(\"%s\\n\" % i)\n",
        "\n",
        "with open('pisma_ignore_links.txt', 'a', encoding='utf-8') as file:\n",
        "    for i in ignore_links:\n",
        "        file.write(\"%s\\n\" % i)\n",
        "\n",
        "with open('pisma_error_links.txt', 'a', encoding='utf-8') as file:\n",
        "    for i in errors:\n",
        "        file.write(\"%s\\n\" % i)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
